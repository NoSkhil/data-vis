{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f570c5a3c6aa910",
   "metadata": {},
   "source": [
    "# Problem Analysis Workshop 2\n",
    "\n",
    "## Group4 Members\n",
    "\n",
    "* Shyam Akhil Nekkanti - 8982123\n",
    "* Jun He (Helena) - 8903073\n",
    "* Zheming Li (Brendan) - 8914152\n",
    "\n",
    "### About Dataset\n",
    "UPDATE: Source code used for collecting this data released here\n",
    "\n",
    "Context\n",
    "YouTube (the world-famous video sharing website) maintains a list of the top trending videos on the platform. According to Variety magazine, “To determine the year’s top-trending videos, YouTube uses a combination of factors including measuring users interactions (number of views, shares, comments and likes). Note that they’re not the most-viewed videos overall for the calendar year”. Top performers on the YouTube trending list are music videos (such as the famously virile “Gangam Style”), celebrity and/or reality TV performances, and the random dude-with-a-camera viral videos that YouTube is well-known for.\n",
    "\n",
    "This dataset is a daily record of the top trending YouTube videos.\n",
    "\n",
    "Note that this dataset is a structurally improved version of this dataset.\n",
    "\n",
    "Content\n",
    "This dataset includes several months (and counting) of data on daily trending YouTube videos. Data is included for the US, GB, DE, CA, and FR regions (USA, Great Britain, Germany, Canada, and France, respectively), with up to 200 listed trending videos per day.\n",
    "\n",
    "EDIT: Now includes data from RU, MX, KR, JP and IN regions (Russia, Mexico, South Korea, Japan and India respectively) over the same time period.\n",
    "\n",
    "Each region’s data is in a separate file. Data includes the video title, channel title, publish time, tags, views, likes and dislikes, description, and comment count.\n",
    "\n",
    "The data also includes a category_id field, which varies between regions. To retrieve the categories for a specific video, find it in the associated JSON. One such file is included for each of the five regions in the dataset.\n",
    "\n",
    "For more information on specific columns in the dataset refer to the column metadata.\n",
    "\n",
    "Acknowledgements\n",
    "This dataset was collected using the YouTube API.\n",
    "\n",
    "Inspiration\n",
    "Possible uses for this dataset could include:\n",
    "\n",
    "Sentiment analysis in a variety of forms\n",
    "Categorising YouTube videos based on their comments and statistics.\n",
    "Training ML algorithms like RNNs to generate their own YouTube comments.\n",
    "Analysing what factors affect how popular a YouTube video will be.\n",
    "Statistical analysis over time\f."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac828bf5cf7989a",
   "metadata": {},
   "source": [
    "## Field of Inquiry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750aebd454f0abbb",
   "metadata": {},
   "source": [
    "Digital media and content analysis, focusing on YouTube trending videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854fd29beb8f7044",
   "metadata": {},
   "source": [
    "## Potential Poorly Defined Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95b11a45090d82",
   "metadata": {},
   "source": [
    "**Do longer videos perform better?**\n",
    "\n",
    "Oversimplifies the relationship between video length and performance. Doesn't consider genre, audience retention, or platform algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ed938d6e5728b",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e3d3eb9fcaa09",
   "metadata": {},
   "source": [
    "What factors most strongly correlate with a video's likelihood of trending on YouTube across different regions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f296f3a24d9c67",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2531d03737518109",
   "metadata": {},
   "source": [
    "### [Trending YouTube Video Statistics](https://www.kaggle.com/datasets/datasnaek/youtube-new)\n",
    "\n",
    "* CSV\n",
    "* Contains data for multiple regions including US, GB, DE, CA, FR, RU, MX, KR, JP and IN.\n",
    "* Includes Various metrics such as views, likes, dislikes, comment count, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19265af88ae850b",
   "metadata": {},
   "source": [
    "## Data Processing （Refine the data source）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cc45a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CAvideos.csv', 'DEvideos.csv', 'FRvideos.csv', 'GBvideos.csv', 'INvideos.csv', 'JPvideos.csv', 'KRvideos.csv', 'MXvideos.csv', 'RUvideos.csv', 'USvideos.csv']\n",
      "['CA', 'DE', 'FR', 'GB', 'IN', 'JP', 'KR', 'MX', 'RU', 'US']\n"
     ]
    }
   ],
   "source": [
    "## step1: get all the csv file\n",
    "import os\n",
    "folder_path = 'youtube-dataset'\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "prefixes =  [f.replace('videos.csv', '') for f in csv_files]\n",
    "print(csv_files)\n",
    "print(prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e061fa0248f3b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAvideos.csv\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 16 fields in line 30884, saw 17\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file, prefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(csv_files, prefixes):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# with open(folder_path+\"/\"+file, 'rb') as f:\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m#     result = cd.detect(f.read())\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# file_encoding = result['encoding']\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# print(file_encoding)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(file)\n\u001b[1;32m---> 13\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43municode_escape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregion\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m prefix  \u001b[38;5;66;03m# Add 'region' column with the prefix value\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     dfs\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "File \u001b[1;32me:\\AI-program\\CSCN8010_ve_Helena\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\AI-program\\CSCN8010_ve_Helena\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\AI-program\\CSCN8010_ve_Helena\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32me:\\AI-program\\CSCN8010_ve_Helena\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 16 fields in line 30884, saw 17\n"
     ]
    }
   ],
   "source": [
    "## step2: assign each file region column\n",
    "## for checking encode method\n",
    "# import chardet as cd\n",
    "import pandas as pd\n",
    "# Process each file, add a 'region' column, and store them in a list\n",
    "dfs = []\n",
    "for file, prefix in zip(csv_files, prefixes):\n",
    "    # with open(folder_path+\"/\"+file, 'rb') as f:\n",
    "    #     result = cd.detect(f.read())\n",
    "    # file_encoding = result['encoding']\n",
    "    # print(file_encoding)\n",
    "    print(file)\n",
    "    df = pd.read_csv(folder_path+\"/\"+file, encoding= 'unicode_escape')\n",
    "    df['region'] = prefix  # Add 'region' column with the prefix value\n",
    "    dfs.append(df)\n",
    "\n",
    "## step3: Concatenate all the dataframes\n",
    "merged_df = pd.concat(dfs)\n",
    "\n",
    "# step4: Save the result to Allvideos.csv\n",
    "merged_df.to_csv(folder_path+'/Allvideos.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the merged dataframe as output\n",
    "print(merged_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eb03d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
